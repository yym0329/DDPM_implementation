{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfPVwCrSxolD"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb1X5GGCCk6U"
      },
      "source": [
        "# Denoising Diffusion Probabilistic Model\n",
        "### [Team 25] Youngmin Ryou, ChulHyun Hwang\n",
        "\n",
        "Main contribution\n",
        "\n",
        "\n",
        "*   Reproduce the results of the paper using CIFAR10 dataset\n",
        "*   experiment the effect of using **adaptive Weight** to the loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdXJksVbHsaL",
        "outputId": "938449f0-2e38-46d2-c015-4469263422e2"
      },
      "outputs": [],
      "source": [
        "!pip install einops\n",
        "!pip install torchmetrics\n",
        "!pip install scipy\n",
        "!pip install torch-fidelity"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set root dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JenR8KnnHJ3",
        "outputId": "6a07517c-13f6-44ba-abd1-3218d33112c9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# Edit this root directory!!! ===================================\n",
        "# root = \"/gdrive/MyDrive/\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## set the result_dir to the directory where the model parameter exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lct_6CHYqqq",
        "outputId": "d4c653f3-28c9-49c0-e9d1-0af6252488b4"
      },
      "outputs": [],
      "source": [
        "import torch, einops\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from time import time\n",
        "\n",
        "# Edit This result directory to the directory where the pretrained weights of the diffuser model exists.\n",
        "# result_dir = r'/gdrive/'\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dptpZBlIUbO"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUi85MWTITpX"
      },
      "outputs": [],
      "source": [
        "def save_model(model, result_dir, mode='last'):\n",
        "    \"\"\"\n",
        "        save model to result.\n",
        "\n",
        "    \"\"\"\n",
        "    torch.save(model.state_dict(), os.path.join(result_dir,  f'ddpm_{mode}.ckpt'))\n",
        "\n",
        "def load_model(model, result_dir, mode='last'):\n",
        "    \"\"\"\n",
        "        load model from result_dir \n",
        "    \"\"\"\n",
        "    if os.path.exists(os.path.join(result_dir , f'ddpm_{mode}.ckpt')):\n",
        "        model.load_state_dict(torch.load(os.path.join(result_dir , f'ddpm_{mode}.ckpt')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydi4Okc0JWx3"
      },
      "source": [
        "### Blocks in Unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBENNNu5ISN8"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_groups=8, dropout=0.1, t_emb_dim = 512, with_attention = False):\n",
        "        \"\"\"\n",
        "            according to the paper, we use resnet block with 2 cnn layers with 1 group normalization layer between the cnn layers.\n",
        "            and use linear layer for positional embedding.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        super(ResBlock, self).__init__()\n",
        "        \n",
        "        self.gn_in = nn.GroupNorm(num_groups, in_channels, affine=True)\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride = 1, padding = 1)\n",
        "        self.with_attention = with_attention\n",
        "        if with_attention: # for 16*16 level of the unet back bone, add attention layer b.w convoluiton layers.\n",
        "            self.attn_gn = nn.GroupNorm(num_groups, out_channels, affine=True)\n",
        "            self.attn = Attention(out_channels)\n",
        "            self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride = 1, padding = 1)\n",
        "        self.gn1 = nn.GroupNorm(num_groups, in_channels, affine=True)\n",
        "        self.gn2 = nn.GroupNorm(num_groups, out_channels, affine=True)\n",
        "        self.act = nn.SiLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # if input channel != output channel, use 1*1 conv to match the shape of the input and output for the residual connection.\n",
        "        self.is_res_proj = (in_channels != out_channels)\n",
        "        if (in_channels != out_channels):\n",
        "            self.res_proj = nn.Conv2d(in_channels, out_channels, kernel_size = 1) \n",
        "\n",
        "        self.t_emb_dim = t_emb_dim\n",
        "        if t_emb_dim is not None:\n",
        "            self.time_proj = nn.Linear(t_emb_dim, out_channels)\n",
        "        \n",
        "    def forward(self, x, pe):\n",
        "        # print(\"h size before con1\", x.shape)\n",
        "        h = self.conv1(self.act(self.gn_in(x)))\n",
        "        if self.t_emb_dim is not None:\n",
        "            # print(\"proj\")\n",
        "            pe = self.time_proj(self.act(pe))\n",
        "        # print(\"h size after con1\", h.shape)\n",
        "        # print(\"forwarding: pe shaep\", pe.shape)\n",
        "        pe=pe.reshape(h.shape[0], h.shape[1], 1, 1)\n",
        "        h = h + pe\n",
        "        if self.with_attention:\n",
        "            h=self.attn_dropout(\n",
        "                self.attn(\n",
        "                    self.act(\n",
        "                        ( \n",
        "                        self.attn_gn(h)\n",
        "                        ))))\n",
        "        \n",
        "        h = self.conv2(self.act( self.gn2(h) ) )\n",
        "        if self.is_res_proj:\n",
        "            x = self.res_proj(x)\n",
        "        out = h + x\n",
        "        return out\n",
        "        \n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, input_channels, num_heads = 8, num_hidden = 32, dropout=0.1):\n",
        "        \"\"\"\n",
        "            multi-head attention block in the transformer paper.\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.num_hidden = num_hidden\n",
        "        self.scaling = num_hidden ** (-1/2)\n",
        "        self.lin_fgh = nn.Linear(input_channels, 3*num_heads*num_hidden)\n",
        "        self.lin_v = nn.Linear(num_heads*num_hidden, input_channels)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, pe=None):\n",
        "        image_size = x.shape[-1]\n",
        "        x = einops.rearrange(x, 'b c h w -> b (h w) c')\n",
        "        fgh = self.lin_fgh(x)\n",
        "        fgh = einops.rearrange(fgh, 'b n c -> b c n')\n",
        "        f, g, h = fgh.chunk(3, dim = 1)\n",
        "\n",
        "        \n",
        "        f = einops.rearrange(f, 'b (head hidden) n -> b head hidden n', head=self.num_heads)\n",
        "        g = einops.rearrange(g, 'b (head hidden) n -> b head hidden n', head=self.num_heads)\n",
        "        h = einops.rearrange(h, 'b (head hidden) n -> b head hidden n', head=self.num_heads)\n",
        "        \n",
        "        \n",
        "        s = einops.einsum(f, g, 'b head hidden n1, b head hidden n2 -> b head n1 n2')*self.scaling\n",
        "        s = F.softmax(s, dim=-1)\n",
        "        s = self.dropout(s)\n",
        "\n",
        "        o = einops.einsum(s, h, 'b head n1 n2, b head hidden n1 -> b head hidden n2')\n",
        "        o = einops.rearrange(o, 'b head hidden n -> b n (head hidden)')\n",
        "        out = self.lin_v(o)\n",
        "        return einops.rearrange(out, 'b (h w) c -> b c h w', h = image_size)\n",
        "\n",
        "\n",
        "class DownScaling(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        \"\"\"\n",
        "        downscaling layer with strided convolution. downscale the size of the image as 1/2\n",
        "        \"\"\"\n",
        "        super(DownScaling, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "    \n",
        "    def forward(self, x, pe=None):\n",
        "        return self.conv(x)\n",
        "\n",
        "class Upscaling(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        \"\"\"\n",
        "        upscaling layer with transpose deconvolution with stride 2\n",
        "        \"\"\"\n",
        "        super(Upscaling, self).__init__()\n",
        "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x, pe=None):\n",
        "        return self.upconv(x)\n",
        "\n",
        "def sinusoidalPosEnc(max_len:int, times, out_channels:int, device=device):\n",
        "    \"\"\"\n",
        "    max_len: int\n",
        "    image_dim: int\n",
        "    times: (b) int list\n",
        "\n",
        "    return: (b, c,)\n",
        "    \"\"\"\n",
        "    \n",
        "    i = torch.arange(0, out_channels // 2).to(device)\n",
        "    denom = 10000 ** (2*i / out_channels)\n",
        "    denom = einops.repeat(denom, \"c -> b c\", b = len(times))\n",
        "    times = times.reshape(len(times),1)\n",
        "    \n",
        "    pe = torch.zeros([len(times), out_channels])\n",
        "    pe[:, 0::2] = torch.sin(times / denom).to(device)\n",
        "    pe[:, 1::2] = torch.cos(times / denom).to(device)\n",
        "    pe = pe.type(torch.float32)\n",
        "    return pe.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAwJvrcYPgkd"
      },
      "outputs": [],
      "source": [
        "class Unet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        in_size: int = 32, \n",
        "        out_size: int = 32,\n",
        "        num_channels=(128, 128, 256, 256, 512,),\n",
        "        num_groups: int=8,\n",
        "        positional_encoding_type: str = \"sinusoidal\",\n",
        "        max_time: int = 1000,\n",
        "        t_emb_dim = 512,\n",
        "        time_embedding_proj = True\n",
        "        ):\n",
        "\n",
        "        \"\"\"\n",
        "        Description\n",
        "            Unet backbone model. get input image and timestep t, and returns the the same dimesion of image output.\n",
        "\n",
        "        Parameters\n",
        "        \n",
        "            in_channels: input image channels\n",
        "            out_channels: output image channels\n",
        "            in_size: input image size (h=w=in_size)\n",
        "            out_size: output image size (h=w=out_size)\n",
        "            num_channels: number of feature channels in Unet layers.\n",
        "            num_groups: parameter to group normalization in each res blocks.\n",
        "            positional_encoding_type: sinusoidal transformer positional encoding or something. currently, only \"sinusoidal\" is implementd.\n",
        "            max_time: the number of time steps in the diffuser model.\n",
        "            t_emb_dim: time embedding dimension used for the learned time embedding. only used when time_embedding_proj = True\n",
        "            time_embedding_proj: if True, use learned linear time embedding projection to encode the time information.\n",
        "        \"\"\"\n",
        "\n",
        "        super(Unet, self).__init__()\n",
        "        self.max_time = max_time\n",
        "\n",
        "        num_dims = len(num_channels)-1\n",
        "        self.t_emb_dim = t_emb_dim\n",
        "\n",
        "    \n",
        "        self.conv1 = nn.Conv2d(in_channels, num_channels[0], 3, stride=1, padding=1)\n",
        "\n",
        "        self.downs = nn.ModuleList([]) # down scaling layers\n",
        "        self.ups = nn.ModuleList([]) # upscaling layers\n",
        "        self.act = nn.SiLU() # the activation function\n",
        "\n",
        "        # define downscaling step\n",
        "        for i in range(1, len(num_channels)-1):\n",
        "            if (in_size / (2**(i-1))) == 16: # 16*16 layer, the paper added attention block between resnet conv blocks.\n",
        "                self.downs.append(\n",
        "                    nn.ModuleList([\n",
        "                        ResBlock(num_channels[i], num_channels[i], t_emb_dim = self.t_emb_dim, with_attention=True),\n",
        "                        ResBlock(num_channels[i], num_channels[i], t_emb_dim = self.t_emb_dim, with_attention=True),\n",
        "                        DownScaling(num_channels[i], num_channels[i+1])\n",
        "                    ])\n",
        "                )\n",
        "            else:\n",
        "                self.downs.append(\n",
        "                    nn.ModuleList([\n",
        "                        ResBlock(num_channels[i], num_channels[i], t_emb_dim = self.t_emb_dim),\n",
        "                        ResBlock(num_channels[i], num_channels[i], t_emb_dim = self.t_emb_dim ),\n",
        "                        DownScaling(num_channels[i], num_channels[i+1]),\n",
        "                    ])\n",
        "                )\n",
        "\n",
        "\n",
        "        # define middle step\n",
        "        self.mid = nn.ModuleList([\n",
        "            ResBlock(num_channels[-1], num_channels[-1], t_emb_dim = self.t_emb_dim),\n",
        "            ResBlock(num_channels[-1], num_channels[-1], t_emb_dim = self.t_emb_dim),\n",
        "            Upscaling(num_channels[-1], num_channels[-2])\n",
        "        ])\n",
        "\n",
        "        # define upscaling step\n",
        "        for i in range(len(num_channels)-2, 0, -1):\n",
        "            if i == (len(num_channels)-1): # 16*16 layer\n",
        "                self.ups.append(\n",
        "                    nn.ModuleList([\n",
        "                        # due to the skip connection, we need 2*num_channels[i] input channels\n",
        "                        ResBlock(num_channels[i]*2, num_channels[i], t_emb_dim = self.t_emb_dim,with_attention=True), \n",
        "                        ResBlock(num_channels[i], num_channels[i], t_emb_dim = self.t_emb_dim,with_attention=True),\n",
        "                        Upscaling(num_channels[i], num_channels[i-1])\n",
        "                    ])\n",
        "                )\n",
        "            elif i == 1:\n",
        "                self.ups.append( # the last layer, we don't need upscaling layer.\n",
        "                    nn.ModuleList([\n",
        "                        # due to the skip connection, we need 2*num_channels[i] input channels\n",
        "                        ResBlock(num_channels[i]*2, num_channels[i], t_emb_dim = self.t_emb_dim,),\n",
        "                        ResBlock(num_channels[i], num_channels[i], t_emb_dim = self.t_emb_dim),\n",
        "                        \n",
        "                    ])\n",
        "                )\n",
        "            else:\n",
        "                self.ups.append(\n",
        "                    nn.ModuleList([\n",
        "                        # due to the skip connection, we need 2*num_channels[i] input channels\n",
        "                        ResBlock(num_channels[i]*2, num_channels[i],t_emb_dim = self.t_emb_dim),\n",
        "                        ResBlock(num_channels[i], num_channels[i],t_emb_dim = self.t_emb_dim),\n",
        "                        Upscaling(num_channels[i], num_channels[i-1])\n",
        "                    ])\n",
        "                )\n",
        "        # last projection\n",
        "        self.last_proj = nn.Conv2d(num_channels[0], out_channels=out_channels, kernel_size = 3, padding = 1)\n",
        "        self.time_encoding_dimension = num_channels[0]\n",
        "        self.time_embedding_proj = time_embedding_proj\n",
        "\n",
        "        if self.time_embedding_proj:\n",
        "            self.time_emb_projection1 = nn.Linear(self.time_encoding_dimension, self.t_emb_dim)\n",
        "            self.time_emb_projection2 = nn.Linear(self.t_emb_dim, self.t_emb_dim)\n",
        "\n",
        "            \n",
        "        \n",
        "    def forward(self, x, t):\n",
        "        # time embedding\n",
        "        pe = sinusoidalPosEnc(self.max_time, t, self.time_encoding_dimension, device)\n",
        "        pe = self.time_emb_projection2(self.act((self.time_emb_projection1(pe))))\n",
        "        # initial projection\n",
        "        h = self.conv1(x)\n",
        "        \n",
        "        # store hidden states during the downscaling, for the skip connection\n",
        "        down_hs = []\n",
        "\n",
        "        for d in self.downs:\n",
        "            for i, l in enumerate(d):\n",
        "                h=l(h, pe)\n",
        "                if i == (len(d)-2):\n",
        "                    down_hs.append(h)\n",
        "        \n",
        "        for l in self.mid:\n",
        "            h = l(h, pe)\n",
        "\n",
        "        for i, u in enumerate(self.ups):\n",
        "            h = torch.cat([h, down_hs[-i-1]], dim=1) # the skip connection\n",
        "            for l in u:\n",
        "                h = l(h, pe)\n",
        "        # last projection\n",
        "        out = self.last_proj(h)\n",
        "        \n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z6Xd4EHMNxW"
      },
      "source": [
        "### Diffuser with forward and backward process\n",
        "forward(x0, t) -> xt, eps\n",
        "\n",
        "predict(xt, t) -> target \n",
        " - target can be 'x0' or 'eps'\n",
        " - predict the target value with the unet model\n",
        "\n",
        "reverse(xt, t) -> x0\n",
        " - starting from xt with time step t, predict x0 with sequential reverse process.\n",
        " - it is for inference, not training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJpoca76Ma3G"
      },
      "outputs": [],
      "source": [
        "class Diffuser(nn.Module):\n",
        "    def __init__(self, model:nn.Module, beta_1:float = 1e-4, beta_T:float=0.02, scheduler=\"linear\", target=\"noise\", is_adaptive_weight=False):\n",
        "        \"\"\"\n",
        "            The diffuser class with the forward step and the reverse step.\n",
        "\n",
        "        Parameters\n",
        "            model: the Unet back bone model\n",
        "            beta_1: the inital beta value at time step 1. we use linear scheduling for the beta value in the forward step.\n",
        "            beta_T: the last beta value at time step T. we use linear scheduling for the beta value in the forward step.\n",
        "            scheduler: The kind of scheduler for the forward step variance. only \"linear\" is implemented.\n",
        "            target: The target that the backbone model will predict. \"noise\" and \"x0\" is available. We didn't test the \"x0\" case. only \"noise\" is tested.\n",
        "            is_adaptive_weight: If True, we use adaptive weight for loss terms during the training phase.\n",
        "        \"\"\"\n",
        "        super(Diffuser, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "        self.is_adaptive_weight = is_adaptive_weight\n",
        "        if is_adaptive_weight:\n",
        "            self.loss_weights = nn.parameter.Parameter(torch.ones((model.max_time,1)), requires_grad=True)\n",
        "\n",
        "        if (target == \"noise\") or (target == \"x0\"):\n",
        "            self.target = target\n",
        "        else:\n",
        "            raise NotImplementedError(f\"target {target} is not implemented. use 'noise' or 'x0'.\")\n",
        "            \n",
        "        self.max_time = model.max_time\n",
        "        self.beta = self.beta_scheduler(beta_1, beta_T, scheduler)\n",
        "        self.beta = self.beta.to(device)\n",
        "        self.alpha = 1 - self.beta\n",
        "        # cummulative products of alphas\n",
        "        self.cum_alpha = torch.cumprod(self.alpha, dim=0)\n",
        "        # square root of cummulative products of alphas\n",
        "        self.sqrt_cum_alpha = self.cum_alpha ** (1/2)\n",
        "        # square root of (1- cummulative products of alphas)\n",
        "        self.sqrt_resid_cum_alpha = ((1-self.cum_alpha)**(1/2))\n",
        "        # reverse diffuse step variance are set to be the same with beta (by paper)\n",
        "        self.variance_reverse = self.beta\n",
        "\n",
        "    def freeze_loss_weights(self):\n",
        "        \"\"\"\n",
        "            Freeze adaptive loss weight of the diffuser.\n",
        "        \"\"\"\n",
        "        if not self.is_adaptive_weight:\n",
        "            raise NotImplementedError(\"there is no loss weights.\")\n",
        "        self.loss_weights.requires_grad = False\n",
        "\n",
        "    def unfreeze_loss_weights(self):\n",
        "        \"\"\"\n",
        "            unfreeze adaptive loss weight of the diffuser.\n",
        "        \"\"\"\n",
        "\n",
        "        if not self.is_adaptive_weight:\n",
        "            raise NotImplementedError(\"there is no loss weights.\")\n",
        "        self.loss_weights.requires_grad = True\n",
        "        \n",
        "\n",
        "    def beta_scheduler(self, beta_1:float, beta_T:float, scheduler=\"linear\"):\n",
        "        \"\"\"\n",
        "            generate the sequence of beta, the variance of each noise step.\n",
        "        \"\"\"\n",
        "        if scheduler != \"linear\":\n",
        "            raise NotImplementedError(f\"the scheduler type {scheduler} is not implemented.\")\n",
        "        assert(beta_1 < beta_T)\n",
        "        return torch.linspace(beta_1, beta_T, self.max_time)\n",
        "        \n",
        "    def get_forward_diffused_image(self, x0, t, return_noise=True):\n",
        "        \"\"\"\n",
        "            return noised image xt from x0.\n",
        "        \"\"\"\n",
        "        stds = self.sqrt_resid_cum_alpha[t].reshape(-1,1,1,1)\n",
        "        noise = torch.randn_like(x0).to(device)\n",
        "        mean_scales = self.sqrt_cum_alpha[t].reshape(-1,1,1,1)\n",
        "        \n",
        "        return (mean_scales*x0 + stds*noise, noise) if return_noise else mean_scales*x0 + stds*noise\n",
        "        \n",
        "    def forward(self, xt, t):\n",
        "        \"\"\"\n",
        "            predict the target (x0 or noise) with the unet backbone model. \n",
        "        Parameters\n",
        "            xt: diffused images at time step t. torch tensor with size (B,C,H,W)\n",
        "            t: time step of xt. torch tensor with size (B,)\n",
        "        \"\"\"\n",
        "        return self.model(xt, t)\n",
        "\n",
        "    def batch_reverse_diffuse(self, xt, t, return_history=False):\n",
        "        \"\"\"\n",
        "            apply reverse process to xt and returns X0.\n",
        "\n",
        "        Parameters\n",
        "            xt: diffused images at time step t. torch tensor with size (B,C,H,W)\n",
        "            t: time step of xt. torch tensor with size (B,). all elements of t must be the same. e.g. t=torch.Tensor([1000,1000,1000,...,1000])\n",
        "            return_history: if True, this function returns X0 and history of the reverse process.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            if self.target == \"x0\":\n",
        "                return self.model(xt, t)\n",
        "            if return_history:\n",
        "                history = []\n",
        "\n",
        "            img = xt\n",
        "            for i in range(t[0].item(), -1, -1):\n",
        "                eps = self.model(img, t)\n",
        "                t = t - 1 \n",
        "\n",
        "                if i == 0:\n",
        "                    img = (img - (1-self.alpha[i])/self.sqrt_resid_cum_alpha[i] * eps ) / (self.alpha[i] ** (1/2))\n",
        "                else:\n",
        "                    z = torch.randn_like(img).cuda()\n",
        "                    img = (img - (1-self.alpha[i])/self.sqrt_resid_cum_alpha[i] * eps ) / (self.alpha[i] ** (1/2)) + (self.variance_reverse[i] ** (1/2))*z\n",
        "                \n",
        "                if return_history:\n",
        "                    history.append(img)\n",
        "                \n",
        "            if return_history:\n",
        "                return img, torch.stack(history)\n",
        "            else:\n",
        "                return img\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxGk3osl2CAa"
      },
      "source": [
        "### Check input and output dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er8K7cPO0swE"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_groups=8, dropout=0.1, t_emb_dim = 512, with_attention = False):\n",
        "        \"\"\"\n",
        "            according to the paper, we use resnet block with 2 cnn layers with 1 group normalization layer between the cnn layers.\n",
        "            and use linear layer for positional embedding.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        super(ResBlock, self).__init__()\n",
        "        \n",
        "        self.gn_in = nn.GroupNorm(num_groups, in_channels, affine=True)\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride = 1, padding = 1)\n",
        "        self.with_attention = with_attention\n",
        "        if with_attention: # for 16*16 level of the unet back bone, add attention layer b.w convoluiton layers.\n",
        "            self.attn_gn = nn.GroupNorm(num_groups, out_channels, affine=True)\n",
        "            self.attn = Attention(out_channels)\n",
        "            self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride = 1, padding = 1)\n",
        "        self.gn1 = nn.GroupNorm(num_groups, in_channels, affine=True)\n",
        "        self.gn2 = nn.GroupNorm(num_groups, out_channels, affine=True)\n",
        "        self.act = nn.SiLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # if input channel != output channel, use 1*1 conv to match the shape of the input and output for the residual connection.\n",
        "        self.is_res_proj = (in_channels != out_channels)\n",
        "        if (in_channels != out_channels):\n",
        "            self.res_proj = nn.Conv2d(in_channels, out_channels, kernel_size = 1) \n",
        "\n",
        "        self.t_emb_dim = t_emb_dim\n",
        "        if t_emb_dim is not None:\n",
        "            self.time_proj = nn.Linear(t_emb_dim, out_channels)\n",
        "        \n",
        "    def forward(self, x, pe):\n",
        "        # print(\"h size before con1\", x.shape)\n",
        "        h = self.conv1(self.act(self.gn_in(x)))\n",
        "        if self.t_emb_dim is not None:\n",
        "            # print(\"proj\")\n",
        "            pe = self.time_proj(self.act(pe))\n",
        "        # print(\"h size after con1\", h.shape)\n",
        "        # print(\"forwarding: pe shaep\", pe.shape)\n",
        "        pe=pe.reshape(h.shape[0], h.shape[1], 1, 1)\n",
        "        h = h + pe\n",
        "        if self.with_attention:\n",
        "            h=self.attn_dropout(\n",
        "                self.attn(\n",
        "                    self.act(\n",
        "                        ( \n",
        "                        self.attn_gn(h)\n",
        "                        ))))\n",
        "        \n",
        "        h = self.conv2(self.act( self.gn2(h) ) )\n",
        "        if self.is_res_proj:\n",
        "            x = self.res_proj(x)\n",
        "        out = h + x\n",
        "        return out\n",
        "        \n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, input_channels, num_heads = 8, num_hidden = 32, dropout=0.1):\n",
        "        \"\"\"\n",
        "            multi-head attention block in the transformer paper.\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.num_hidden = num_hidden\n",
        "        self.scaling = num_hidden ** (-1/2)\n",
        "        self.lin_fgh = nn.Linear(input_channels, 3*num_heads*num_hidden)\n",
        "        self.lin_v = nn.Linear(num_heads*num_hidden, input_channels)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, pe=None):\n",
        "        image_size = x.shape[-1]\n",
        "        x = einops.rearrange(x, 'b c h w -> b (h w) c')\n",
        "        fgh = self.lin_fgh(x)\n",
        "        fgh = einops.rearrange(fgh, 'b n c -> b c n')\n",
        "        f, g, h = fgh.chunk(3, dim = 1)\n",
        "\n",
        "        \n",
        "        f = einops.rearrange(f, 'b (head hidden) n -> b head hidden n', head=self.num_heads)\n",
        "        g = einops.rearrange(g, 'b (head hidden) n -> b head hidden n', head=self.num_heads)\n",
        "        h = einops.rearrange(h, 'b (head hidden) n -> b head hidden n', head=self.num_heads)\n",
        "        \n",
        "        \n",
        "        s = einops.einsum(f, g, 'b head hidden n1, b head hidden n2 -> b head n1 n2')*self.scaling\n",
        "        s = F.softmax(s, dim=-1)\n",
        "        s = self.dropout(s)\n",
        "\n",
        "        o = einops.einsum(s, h, 'b head n1 n2, b head hidden n1 -> b head hidden n2')\n",
        "        o = einops.rearrange(o, 'b head hidden n -> b n (head hidden)')\n",
        "        out = self.lin_v(o)\n",
        "        return einops.rearrange(out, 'b (h w) c -> b c h w', h = image_size)\n",
        "\n",
        "\n",
        "class DownScaling(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        \"\"\"\n",
        "        downscaling layer with strided convolution. downscale the size of the image as 1/2\n",
        "        \"\"\"\n",
        "        super(DownScaling, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "    \n",
        "    def forward(self, x, pe=None):\n",
        "        return self.conv(x)\n",
        "\n",
        "class Upscaling(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        \"\"\"\n",
        "        upscaling layer with transpose deconvolution with stride 2\n",
        "        \"\"\"\n",
        "        super(Upscaling, self).__init__()\n",
        "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x, pe=None):\n",
        "        return self.upconv(x)\n",
        "\n",
        "def sinusoidalPosEnc(max_len:int, times, out_channels:int, device=device):\n",
        "    \"\"\"\n",
        "    max_len: int\n",
        "    image_dim: int\n",
        "    times: (b) int list\n",
        "\n",
        "    return: (b, c,)\n",
        "    \"\"\"\n",
        "    \n",
        "    i = torch.arange(0, out_channels // 2).to(device)\n",
        "    denom = 10000 ** (2*i / out_channels)\n",
        "    denom = einops.repeat(denom, \"c -> b c\", b = len(times))\n",
        "    times = times.reshape(len(times),1)\n",
        "    \n",
        "    pe = torch.zeros([len(times), out_channels])\n",
        "    pe[:, 0::2] = torch.sin(times / denom).to(device)\n",
        "    pe[:, 1::2] = torch.cos(times / denom).to(device)\n",
        "    pe = pe.type(torch.float32)\n",
        "    return pe.to(device)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rWQSCOEWohrs"
      },
      "source": [
        "## Load diffuser\n",
        "You need to choose whether you will use adaptive loss or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05uJlam8ahPi",
        "outputId": "13188a3b-118b-473a-8b33-22b4726b3ff8"
      },
      "outputs": [],
      "source": [
        "# If you are training adaptive weight diffuser, decomment below\n",
        "\"\"\"\n",
        "model = Unet(3,3, ).to(device)\n",
        "diffuser = Diffuser(model, is_adaptive_weight=True).to(device)\n",
        "\"\"\"\n",
        "# else, decomment below:\n",
        "model = Unet(3,3, ).to(device)\n",
        "diffuser = Diffuser(model, is_adaptive_weight=True).to(device)\n",
        "\n",
        "\n",
        "# Report the number of trainable parameters\n",
        "trainable_params = sum(\n",
        "\tp.numel() for p in diffuser.parameters() if p.requires_grad\n",
        ")\n",
        "print(trainable_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rc2kSh4husnN"
      },
      "outputs": [],
      "source": [
        "load_model(diffuser, result_dir, 'best')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "POB4NP15roHP"
      },
      "source": [
        "Draw predetermined weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "GBpdxSEZrp-3",
        "outputId": "e346f35c-2fde-4b6e-bf8d-784e3e9f4186"
      },
      "outputs": [],
      "source": [
        "vb_weights = (diffuser.beta / (2*diffuser.alpha*(1-diffuser.cum_alpha)))\n",
        "\n",
        "plt.figure(figsize=(20,4))\n",
        "weights = vb_weights\n",
        "weight = ((weights.to('cpu')).detach()).numpy()\n",
        "plt.plot(weight)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmhaxxbsIais"
      },
      "source": [
        "Generate Progressive generation figures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "p9fHFryziNOR",
        "outputId": "1960984f-4dec-43ce-f5e3-253e07601073"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.functional import InterpolationMode\n",
        "num_batches = 5\n",
        "num_show = 20\n",
        "fig, axs = plt.subplots(11, num_show, figsize=(3*num_show, 30))\n",
        "for j in range(num_batches):\n",
        "    noise = torch.randn((num_show,3,32,32)).to(device)\n",
        "    t = (999*torch.ones((num_show,)).cuda()).type(torch.int32)\n",
        "\n",
        "    t1 = time()\n",
        "    img, his = diffuser.batch_reverse_diffuse(noise, t, True)\n",
        "    print(\"time spent on generation: \", time()-t1)\n",
        "    image = his[999,:,:,:].to('cpu')\n",
        "    \n",
        "\n",
        "    for k in range(10):\n",
        "        for i, image in enumerate(his[k*100,:,:,:].to('cpu')):\n",
        "            if i == num_show:\n",
        "                break\n",
        "            axs[k][i].imshow(((image.permute(1, 2, 0)+1)*127.5).type(torch.uint8).numpy() )\n",
        "            axs[k][i].set_axis_off()\n",
        "            \n",
        "\n",
        "    for i, image in enumerate(his[999,:,:,:].to('cpu')):\n",
        "        if i == num_show:\n",
        "                break\n",
        "        axs[10][i].imshow(((image.permute(1, 2, 0)+1)*127.5).type(torch.uint8).numpy())\n",
        "        axs[10][i].set_axis_off()\n",
        "\n",
        "    fig.show()\n",
        "    fig.savefig(os.path.join(result_dir, f\"progressive_{j}.png\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqtDAmDaqoVw"
      },
      "source": [
        "Generate `gen_batch * num_batches` images in `gen_img_path`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umeQEXnfIh5a",
        "outputId": "ad3bf7cf-e8b2-4393-f49b-6ec39b5a3650"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import save_image\n",
        "from datetime import timedelta\n",
        "\n",
        "\n",
        "gen_batch = 256\n",
        "num_batches = 40\n",
        "gen_img_path = os.path.join(result_dir, 'sample_imgs')\n",
        "\n",
        "if not os.path.exists(gen_img_path):\n",
        "    os.mkdir(gen_img_path)\n",
        "    print(\"create directory: \", gen_img_path)\n",
        "print(f\"generate {gen_batch * num_batches} samples in \", gen_img_path)\n",
        "\n",
        "for i in range(num_batches):\n",
        "    noise = torch.randn((gen_batch,3,32,32)).to(device)\n",
        "    t = (999*torch.ones((gen_batch,)).cuda()).type(torch.int32)\n",
        "    t1 = time()\n",
        "    img, his = diffuser.batch_reverse_diffuse(noise, t, True)\n",
        "    image = his[999,:,:,:].to('cpu')\n",
        "    for j, img in enumerate(image):\n",
        "        save_image(img, os.path.join( gen_img_path ,f\"image_{gen_batch*i+j}.png\"))\n",
        "    ellapsed_time = time() - t1\n",
        "    print(f\"{i} the epoch time spent: {ellapsed_time}\", f\"time left: {timedelta(seconds=ellapsed_time * (num_batches - i))}\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "YMBi-11CpUjw",
        "outputId": "be3e05ca-d9a4-47b6-ab71-d6546abf1add"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "from torchmetrics.image.inception import InceptionScore\n",
        "from torchvision.io import read_image\n",
        "import os, torch\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import  Compose, ToTensor, Lambda\n",
        "\n",
        "image_names = os.listdir(gen_img_path)\n",
        "image_list = []\n",
        "\n",
        "for f in image_names:\n",
        "    image_list.append(read_image(os.path.join(gen_img_path, f)).cuda())\n",
        "gen_images = torch.stack(image_list)\n",
        "\n",
        "\n",
        "transform = Compose(\n",
        "    [\n",
        "        ToTensor(),\n",
        "        Lambda(lambda x: x.to(device)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "cifar10_test = CIFAR10(root, train=False, transform = transform,download=True)\n",
        "\n",
        "real_imgs = torch.from_numpy(cifar10_test.data).cuda()\n",
        "real_imgs = real_imgs.permute(0,3,1,2)\n",
        "\n",
        "fid = FrechetInceptionDistance(feature=2048).to(device)\n",
        "inception_score = InceptionScore().to(device)\n",
        "print(\"FID device: \", fid.device)\n",
        "print(\"IS device: \", inception_score.device)\n",
        "\n",
        "batch_size = 128\n",
        "num_batches = (gen_images.shape)[0] // batch_size \n",
        "gen_images[:128].shape\n",
        "\n",
        "for i in range(num_batches):\n",
        "    fid.update(gen_images[batch_size*i:batch_size*(i+1)], real=False)\n",
        "    inception_score.update(gen_images[batch_size*i:batch_size*(i+1)])\n",
        "\n",
        "if gen_images.shape[0] % 128 != 0:\n",
        "    fid.update(gen_images[batch_size*num_batches:], real=False)    \n",
        "    inception_score.update(gen_images[batch_size*num_batches:])\n",
        "\n",
        "num_batches = real_imgs.shape[0] // batch_size\n",
        "for i in range(num_batches):\n",
        "    fid.update(real_imgs[batch_size*i:batch_size*(i+1)], real=True)\n",
        "fid.update(real_imgs[batch_size*num_batches:], real=True)\n",
        "\n",
        "fid_res = fid.compute()\n",
        "is_res = inception_score.compute()\n",
        "print(f\"FID score: {fid_res.item()}\")\n",
        "print(f\"Inception score: {is_res[0].item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9yLAA2Lve2h"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MBWytjXg74I"
      },
      "outputs": [],
      "source": [
        "def interpolation(diffuser, image1, image2, weight_on_img1, latent_time_step):\n",
        "    \"\"\"\n",
        "        diffuser: diffuser model to do latent transform and inversion.\n",
        "        image1: torch.tensor with size (h,w,3). [-1,1] normalized.\n",
        "        image2: torch.tensor with size (h,w,3). [-1,1] normalized.\n",
        "        weight_on_img1: torch.float, between 0 and 1. \n",
        "        latent_time_step: diffuser forwards the input image to this time step.\n",
        "    \"\"\"\n",
        "    time = torch.tensor([t]).to(device)\n",
        "    images = torch.stack([image1, image2]).to(device)\n",
        "    noised_image = diffuser.get_forward_diffused_image(images, time)\n",
        "    \n",
        "    itp_on_latent_space = weight_on_img1*noised_image[0] + (1-weight_on_img1)*noised_image[1]\n",
        "    reversed = diffuser.batch_reverse_diffuse(itp_on_latent_space.unsqueeze(0), time)\n",
        "    return reversed\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
